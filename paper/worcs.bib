
@article{aalbersbergMakingScienceTransparent2018,
  title = {Making {{Science Transparent By Default}}; {{Introducing}} the {{TOP Statement}}},
  author = {Aalbersberg, IJsbrand Jan and Appleyard, Tom and Brookhart, Sarah and Carpenter, Todd and Clarke, Michael and Curry, Stephen and Dahl, Josh and DeHaven, Alexander Carl and Eich, Eric and Franko, Maryrose and Freedman, Len and Graf, Chris and Grant, Sean and Hanson, Brooks and Joseph, Heather and Kiermer, Veronique and Kramer, Bianca and Kraut, Alan and Karn, Roshan Kumar and Lee, Carole and MacFarlane, Aki and Martone, Maryann and {Mayo-Wilson}, Evan and McNutt, Marcia and McPhail, Meredith and Mellor, David Thomas and Moher, David and Mudditt, Alison and Nosek, Brian A. and Orland, Belinda and Parker, Timothy H. and Parsons, Mark and Patterson, Mark and Santos, Solange and Shore, Carolyn and Simons, Daniel J. and Spellman, Bobbie and Spies, Jeffrey Robert and Spitzer, Matthew and Stodden, Victoria and Swaminathan, Sowmya and Sweet, Deborah and Tsui, Anne and Vazire, Simine},
  year = {2018},
  month = feb,
  doi = {10.31219/osf.io/sm78t},
  abstract = {In order to increase the replicability of scientific work, the scientific community has called for practices designed to increase the transparency of research (McNutt, 2014; Nosek et al., 2015). The validity of a scientific claim depends not on the reputation of those making the claim, the venue in which the claim is made, or the novelty of the result, but rather on the empirical evidence provided by the underlying data and methods. Proper evaluation of  the merits of scientific findings requires availability of the methods, materials, and data and the reasoned argument that serve as the basis for the published conclusions (Claerbout and Karrenbach 1992; Donoho et al 2009; Stodden et al 2013; Borwein et al 2013; Munaf\`o et al, 2017). Wide and growing support for these principles (see, for example, signatories to Declaration on Research Assessment, DORA, https://sfdora.org/, and the Transparency and Openness Promotion Guidelines https://cos.io/our-services/top-guidelines/) must be coupled with guidelines to increase open sharing of data and research materials, use of reporting guidelines, preregistration, and replication. We propose that, going forward, authors of all scientific articles disclose the availability and location of all research items, including data, materials, and code, related to their published articles in what we will refer to as a TOP Statement.}
}

@article{aczelConsensusbasedTransparencyChecklist2019,
  title = {A Consensus-Based Transparency Checklist},
  author = {Aczel, Balazs and Szaszi, Barnabas and Sarafoglou, Alexandra and Kekecs, Zoltan and Kucharsk{\'y}, {\v S}imon and Benjamin, Daniel and Chambers, Christopher D. and Fisher, Agneta and Gelman, Andrew and Gernsbacher, Morton A. and Ioannidis, John P. and Johnson, Eric and Jonas, Kai and Kousta, Stavroula and Lilienfeld, Scott O. and Lindsay, D. Stephen and Morey, Candice C. and Munaf{\`o}, Marcus and Newell, Benjamin R. and Pashler, Harold and Shanks, David R. and Simons, Daniel J. and Wicherts, Jelte M. and Albarracin, Dolores and Anderson, Nicole D. and Antonakis, John and Arkes, Hal R. and Back, Mitja D. and Banks, George C. and Beevers, Christopher and Bennett, Andrew A. and Bleidorn, Wiebke and Boyer, Ty W. and Cacciari, Cristina and Carter, Alice S. and Cesario, Joseph and Clifton, Charles and Conroy, Ron{\'a}n M. and Cortese, Mike and Cosci, Fiammetta and Cowan, Nelson and Crawford, Jarret and Crone, Eveline A. and Curtin, John and Engle, Randall and Farrell, Simon and Fearon, Pasco and Fichman, Mark and Frankenhuis, Willem and Freund, Alexandra M. and Gaskell, M. Gareth and {Giner-Sorolla}, Roger and Green, Don P. and Greene, Robert L. and Harlow, Lisa L. and de la Guardia, Fernando Hoces and Isaacowitz, Derek and Kolodner, Janet and Lieberman, Debra and Logan, Gordon D. and Mendes, Wendy B. and Moersdorf, Lea and Nyhan, Brendan and Pollack, Jeffrey and Sullivan, Christopher and Vazire, Simine and Wagenmakers, Eric-Jan},
  year = {2019},
  month = dec,
  pages = {1--3},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0772-6},
  abstract = {We present a consensus-based checklist to improve and document the transparency of research reports in social and behavioural research. An accompanying online application allows users to complete the form and generate a report that they can submit with their manuscript or post to a public repository.},
  copyright = {2019 The Author(s)},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\5T92PG4Y\\Aczel et al_2019_A consensus-based transparency checklist.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\ZLCD57B9\\s41562-019-0772-6.html},
  journal = {Nature Human Behaviour},
  language = {en}
}

@article{adolphOpenBehavioralScience2012,
  title = {Toward {{Open Behavioral Science}}},
  author = {Adolph, Karen E. and Gilmore, Rick O. and Freeman, Clinton and Sanderson, Penelope and Millman, David},
  year = {2012},
  month = jul,
  volume = {23},
  pages = {244--247},
  issn = {1047-840X, 1532-7965},
  doi = {10.1080/1047840X.2012.705133},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\PYPE6J4K\\Adolph et al. - 2012 - Toward Open Behavioral Science.pdf},
  journal = {Psychological Inquiry},
  keywords = {open developmental science},
  language = {en},
  number = {3}
}

@misc{allaireMarkdownPythonEngine,
  title = {R {{Markdown Python Engine}}},
  author = {Allaire, J. J. and Ushey, Kevin and RStudio and Tang, Yuan},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\WSHC7J3P\\r_markdown.html},
  howpublished = {https://rstudio.github.io/reticulate/articles/r\_markdown.html}
}

@book{allaireRticlesArticleFormats2020,
  title = {Rticles: {{Article}} Formats for r Markdown},
  author = {Allaire, JJ and Xie, Yihui and {R Foundation} and Wickham, Hadley and {Journal of Statistical Software} and Vaidyanathan, Ramnath and {Association for Computing Machinery} and Boettiger, Carl and {Elsevier} and Broman, Karl and Mueller, Kirill and Quast, Bastiaan and Pruim, Randall and Marwick, Ben and Wickham, Charlotte and Keyes, Oliver and Yu, Miao and Emaasit, Daniel and Onkelinx, Thierry and Gasparini, Alessandro and Desautels, Marc-Andre and Leutnant, Dominik and {MDPI} and {Taylor and Francis} and {\"O}greden, Oguzhan and Hance, Dalton and N{\"u}st, Daniel and Uvesten, Petter and Campitelli, Elio and Muschelli, John and Kamvar, Zhian N. and Ross, Noam and Cannoodt, Robrecht and Luguern, Duncan and Kaplan, David M.},
  year = {2020}
}

@misc{austPapajaPrepareReproducible2020,
  title = {Papaja: {{Prepare}} Reproducible {{APA}} Journal Articles with {{R Markdown}}.},
  author = {Aust, Frederik and Barth, Marius},
  year = {2020},
  month = jan,
  abstract = {papaja (Preparing APA Journal Articles) is an R package that provides document formats to produce complete APA manscripts from RMarkdown-files (PDF and Word documents) and helper functions that fac...},
  keywords = {apa,apa-guidelines,journal,manuscript,psychology,r,reproducible-paper,reproducible-research,rmarkdown}
}

@misc{austPreregMarkdownTemplates2019,
  title = {Prereg: {{R Markdown Templates}} to {{Preregister Scientific Studies}}},
  shorttitle = {Prereg},
  author = {Aust, Frederik},
  year = {2019},
  month = jan,
  abstract = {Provides a collection of templates to author preregistration documents for scientific studies in PDF format.},
  copyright = {GPL-3}
}

@article{bauerExpandingReachPsychological2019,
  title = {Expanding the {{Reach}} of {{Psychological Science}}},
  author = {Bauer, Patricia J.},
  year = {2019},
  month = dec,
  pages = {0956797619898664},
  issn = {0956-7976},
  doi = {10.1177/0956797619898664},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\RIZC4MV4\\2019_Expanding the Reach of Psychological Science.pdf},
  journal = {Psychological Science},
  language = {en}
}

@article{blischakQuickIntroductionVersion2016,
  title = {A {{Quick Introduction}} to {{Version Control}} with {{Git}} and {{GitHub}}},
  author = {Blischak, John D. and Davenport, Emily R. and Wilson, Greg},
  year = {2016},
  month = jan,
  volume = {12},
  pages = {e1004668},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004668},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\NYSUERQ3\\Blischak et al_2016_A Quick Introduction to Version Control with Git and GitHub.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\BCILHZ8S\\article.html},
  journal = {PLOS Computational Biology},
  keywords = {Cloning,Control systems,Graphical user interface,Kidneys,Machine learning,Morphology (linguistics),Scientists,Software development},
  language = {en},
  number = {1}
}

@unpublished{brinkmanWidescaleAdoptionOpen2020,
  title = {Towards {{Wide}}-Scale {{Adoption}} of {{Open}}  {{Science Practices}}: {{The Role}} of {{Bottom}}-up  {{Open Science Communities}}},
  author = {Brinkman, Loek and Armeni, Kristijan and Schettino, Antonio and Eerland, Anita and Heininga, Vera E and Fijten, Rianne and Sjoerds, Zsuzsika and Masselink, Maurits},
  year = {2020},
  abstract = {Open science (OS) increases the quality, efficiency, and impact of science. This has been recognised by scholars, funders, and policymakers worldwide. However, despite the increasing availability of infrastructure supporting OS and the rise in policies and incentives to change behavior, OS practices are often not the norm. While pioneering researchers are developing and embracing OS practices, the majority sticks to the status quo. To transition from pioneering to common practice, we need to engage a critical proportion of the academic community. In this transition, open science communities (OSCs) play a key role. OSCs are bottom-up learning groups of scholars that discuss OS practices, within and across disciplines. They make OS knowledge and know-how more visible and accessible, and facilitate communication amongst scholars and policy makers. By the same token, these scholars also engage in shaping the transition to OS such that it is most beneficial for researchers, science, and society. Over the past two years, ten OSCs were founded in several Dutch university cities, with over 400 members in total. In other countries, scholars are starting similar OSCs. In this paper, we discuss the pivotal role OSCs can play in the large-scale transition to OS and provide practical information to start your own local OSC. We stress that, despite the grassroots character of OSCs, support from universities is critical for OSCs to be viable, effective, and sustainable.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\6SPVL5S6\\Brinkman et al. - Open Science Communities Netherlands (OSC-NL).pdf},
  language = {en}
}

@misc{brownHowLearnedStop2017,
  title = {How {{I}} Learned to Stop Worrying and Love the Coming Archivability Crisis in Scientific Software},
  author = {Brown, C. Titus},
  year = {2017},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\LRI2PYYL\\2017-pof-software-archivability.html},
  howpublished = {http://ivory.idyll.org/blog/2017-pof-software-archivability.html}
}

@article{coyneReplicationInitiativesWill2016,
  title = {Replication Initiatives Will Not Salvage the Trustworthiness of Psychology},
  author = {Coyne, James C.},
  year = {2016},
  month = may,
  volume = {4},
  pages = {28},
  issn = {2050-7283},
  doi = {10.1186/s40359-016-0134-3},
  abstract = {Replication initiatives in psychology continue to gather considerable attention from far outside the field, as well as controversy from within. Some accomplishments of these initiatives are noted, but this article focuses on why they do not provide a general solution for what ails psychology. There are inherent limitations to mass replications ever being conducted in many areas of psychology, both in terms of their practicality and their prospects for improving the science. Unnecessary compromises were built into the ground rules for design and publication of the Open Science Collaboration: Psychology that undermine its effectiveness. Some ground rules could actually be flipped into guidance for how not to conduct replications. Greater adherence to best publication practices, transparency in the design and publishing of research, strengthening of independent post-publication peer review and firmer enforcement of rules about data sharing and declarations of conflict of interest would make many replications unnecessary. Yet, it has been difficult to move beyond simple endorsement of these measures to consistent implementation. Given the strong institutional support for questionable publication practices, progress will depend on effective individual and collective use of social media to expose lapses and demand reform. Some recent incidents highlight the necessity of this.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\FBLTNNET\\Coyne - 2016 - Replication initiatives will not salvage the trust.pdf},
  journal = {BMC Psychology},
  keywords = {open developmental science,replication},
  number = {1}
}

@misc{dariah-campusOpenScienceJust2019,
  title = {Open {{Science}} Is {{Just Good Science}}},
  author = {{DARIAH-Campus}},
  year = {2019},
  month = aug,
  abstract = {DARIAH-CAMPUS is a discovery framework and hosting platform for DARIAH learning resources. Currently in beta.},
  howpublished = {https://campus.dariah.eu/resource/open-science-is-just-good-science},
  journal = {DARIAH Campus},
  language = {en}
}

@misc{EditorsPsychOpen,
  title = {For {{Editors}}: {{PsychOpen}}},
  howpublished = {https://www.psychopen.eu/for-editors/}
}

@article{gauMoreBrainDroppings,
  title = {More Brain Droppings on the Replication Crisis},
  author = {Gau, Remi},
  pages = {52},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\IEXXQU7X\\Gau - More brain droppings on the replication crisis.pdf},
  language = {en}
}

@article{gelmanStatisticalCrisisScience2014,
  title = {The Statistical Crisis in Science: Data-Dependent Analysis--a \&quot;Garden of Forking Paths\&quot;--Explains Why Many Statistically Significant Comparisons Don't Hold Up},
  shorttitle = {The Statistical Crisis in Science},
  author = {Gelman, Andrew and Loken, Eric},
  year = {2014},
  month = nov,
  volume = {102},
  pages = {460--466},
  issn = {00030996},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\M37N35RL\\anonymous.html},
  journal = {American Scientist},
  language = {English},
  number = {6}
}

@book{grolemundDataScience2017,
  title = {R for {{Data Science}}},
  author = {Grolemund, Garrett and Wickham, Hadley},
  year = {2017},
  publisher = {{O'Reilly}},
  abstract = {This book will teach you how to do data science with R: You'll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you'll learn how to clean data and draw plots\textemdash and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You'll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You'll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\8LSGEMNP\\r4ds.had.co.nz.html}
}

@misc{hallquistMplusAutomationPackageFacilitating2018,
  title = {{{MplusAutomation}}: {{An R Package}} for {{Facilitating Large}}-{{Scale Latent Variable Analyses}} in {{Mplus}}},
  shorttitle = {{{MplusAutomation}}},
  author = {Hallquist, Michael and Wiley, Joshua and van Lissa, Caspar},
  year = {2018},
  month = nov,
  abstract = {Leverages the R language to automate latent variable model estimation and interpretation using 'Mplus', a powerful latent variable modeling program developed by Muthen and Muthen ({$<$}http://www.statmodel.com{$>$}). Specifically, this package provides routines for creating related groups of models, running batches of models, and extracting and tabulating model parameters and fit statistics.},
  copyright = {LGPL-3},
  keywords = {Psychometrics}
}

@article{helgessonResponsibilityScientificMisconduct2018,
  title = {Responsibility for Scientific Misconduct in Collaborative Papers},
  author = {Helgesson, Gert and Eriksson, Stefan},
  year = {2018},
  month = sep,
  volume = {21},
  pages = {423--430},
  issn = {1572-8633},
  doi = {10.1007/s11019-017-9817-7},
  abstract = {This paper concerns the responsibility of co-authors in cases of scientific misconduct. Arguments in research integrity guidelines and in the bioethics literature concerning authorship responsibilities are discussed. It is argued that it is unreasonable to claim that for every case where a research paper is found to be fraudulent, each author is morally responsible for all aspects of that paper, or that one particular author has such a responsibility. It is further argued that it is more constructive to specify what task responsibilities come with different roles in a project and describe what kinds of situations or events call for some kind of action, and what the appropriate actions might be.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\SK7V3ZIB\\Helgesson_Eriksson_2018_Responsibility for scientific misconduct in collaborative papers.pdf},
  journal = {Medicine, Health Care and Philosophy},
  keywords = {Accountability,Authorship,Research ethics,Research integrity,Responsibility,Scientific misconduct},
  language = {en},
  number = {3}
}

@article{johnMeasuringPrevalenceQuestionable2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  month = may,
  volume = {23},
  pages = {524--532},
  issn = {0956-7976},
  doi = {10.1177/0956797611430953},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\TJKTAI5M\\John et al_2012_Measuring the Prevalence of Questionable Research Practices With Incentives for.pdf},
  journal = {Psychological Science},
  keywords = {disclosure,judgment,methodology,professional standards},
  language = {en},
  number = {5}
}

@article{kerrHARKingHypothesizingResults1998,
  title = {{{HARKing}}: Hypothesizing after the Results Are Known},
  shorttitle = {{{HARKing}}},
  author = {Kerr, N. L.},
  year = {1998},
  volume = {2},
  pages = {196--217},
  issn = {1088-8683},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as i f it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing ' s costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\N57L34VT\\Kerr - 1998 - HARKing hypothesizing after the results are known.pdf},
  journal = {Personality and Social Psychology Review: An Official Journal of the Society for Personality and Social Psychology, Inc},
  language = {eng},
  number = {3},
  pmid = {15647155}
}

@article{kidwellBadgesAcknowledgeOpen2016,
  title = {Badges to {{Acknowledge Open Practices}}: {{A Simple}}, {{Low}}-{{Cost}}, {{Effective Method}} for {{Increasing Transparency}}},
  shorttitle = {Badges to {{Acknowledge Open Practices}}},
  author = {Kidwell, Mallory C. and Lazarevi{\'c}, Ljiljana B. and Baranski, Erica and Hardwicke, Tom E. and Piechowski, Sarah and Falkenberg, Lina-Sophia and Kennett, Curtis and Slowik, Agnieszka and Sonnleitner, Carina and {Hess-Holden}, Chelsey and Errington, Timothy M. and Fiedler, Susann and Nosek, Brian A.},
  year = {2016},
  month = may,
  volume = {14},
  pages = {e1002456},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002456},
  abstract = {Beginning January 2014, Psychological Science gave authors the opportunity to signal open data and materials if they qualified for badges that accompanied published articles. Before badges, less than 3\% of Psychological Science articles reported open data. After badges, 23\% reported open data, with an accelerating trend; 39\% reported open data in the first half of 2015, an increase of more than an order of magnitude from baseline. There was no change over time in the low rates of data sharing among comparison journals. Moreover, reporting openness does not guarantee openness. When badges were earned, reportedly available data were more likely to be actually available, correct, usable, and complete than when badges were not earned. Open materials also increased to a weaker degree, and there was more variability among comparison journals. Badges are simple, effective signals to promote open practices and improve preservation of data and materials by using independent repositories.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\35AQ9U5U\\Kidwell et al. - 2016 - Badges to Acknowledge Open Practices A Simple, Lo.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\NGFUB647\\article.html},
  journal = {PLOS Biology},
  keywords = {Behavior,Cognitive psychology,Experimental psychology,Open data,open developmental science,Open science,Psychology,Research assessment,Scientific publishing},
  language = {en},
  number = {5}
}

@article{klebelPeerReviewPreprint2020,
  title = {Peer Review and Preprint Policies Are Unclear at Most Major Journals},
  author = {Klebel, Thomas and Reichmann, Stefan and Polka, Jessica and McDowell, Gary and Penfold, Naomi and Hindle, Samantha and {Ross-Hellauer}, Tony},
  year = {2020},
  month = jan,
  pages = {2020.01.24.918995},
  doi = {10.1101/2020.01.24.918995},
  abstract = {{$<$}p{$>$}Clear and findable publishing policies are important for authors to choose appropriate journals for publication. We investigated the clarity of policies of 171 major academic journals across disciplines regarding peer review and preprinting. 31.6\% of journals surveyed do not provide information on the type of peer review they use. Information on whether preprints can be posted or not is unclear in 39.2\% of journals. 58.5\% of journals offer no clear information on whether reviewer identities are revealed to authors. Around 75\% of journals have no clear policy on coreviewing, citation of preprints, and publication of reviewer identities. Information regarding practices of Open Peer Review is even more scarce, with \&lt;20\% of journals providing clear information. Having found a lack of clear information, we conclude by examining the implications this has for researchers (especially early career) and the spread of open research practices.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\CZ5JNZPP\\Klebel et al_2020_Peer review and preprint policies are unclear at most major journals.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\LCSN629R\\2020.01.24.html},
  journal = {bioRxiv},
  language = {en}
}

@article{kulkeImplicitTheoryMind2018,
  title = {Is {{Implicit Theory}} of {{Mind}} a {{Real}} and {{Robust Phenomenon}}? {{Results From}} a {{Systematic Replication Study}}},
  shorttitle = {Is {{Implicit Theory}} of {{Mind}} a {{Real}} and {{Robust Phenomenon}}?},
  author = {Kulke, Louisa and {von Duhn}, Britta and Schneider, Dana and Rakoczy, Hannes},
  year = {2018},
  month = jun,
  volume = {29},
  pages = {888--900},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797617747090},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\HRUN9XAA\\Kulke et al. - 2018 - Is Implicit Theory of Mind a Real and Robust Pheno.pdf},
  journal = {Psychological Science},
  keywords = {open developmental science,replication},
  language = {en},
  number = {6}
}

@article{lamprechtFAIRPrinciplesResearch2019,
  title = {Towards {{FAIR}} Principles for Research Software},
  author = {Lamprecht, Anna-Lena and Garcia, Leyla and Kuzak, Mateusz and Martinez, Carlos and Arcila, Ricardo and Martin Del Pico, Eva and Dominguez Del Angel, Victoria and {van de Sandt}, Stephanie and Ison, Jon and Martinez, Paula Andrea and McQuilton, Peter and Valencia, Alfonso and Harrow, Jennifer and Psomopoulos, Fotis and Gelpi, Josep Ll. and Chue Hong, Neil and Goble, Carole and {Capella-Gutierrez}, Salvador},
  editor = {Groth, Paul},
  year = {2019},
  month = nov,
  pages = {1--23},
  issn = {24518492, 24518484},
  doi = {10.3233/DS-190026},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\4YVQJARQ\\Lamprecht et al_2019_Towards FAIR principles for research software.pdf},
  journal = {Data Science}
}

@techreport{leveltFailingScienceFraudulent2012,
  title = {Failing Science: {{The}} Fraudulent Research Practices of Social Psychologist {{Diederik Stapel}} ({{Falende}} Wetenschap: {{De}} Frauduleuze Onderzoekspraktijken van Sociaal-Psycholoog {{Diederik Stapel}})},
  author = {Levelt, J. M., Willem and Noort,, E. and Drenth, P. J. D.},
  year = {2012},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\9R88L72R\\eindrapport_stapel_nov_2012.pdf}
}

@article{lindsayResearchPreregistration1012016,
  title = {Research {{Preregistration}} 101},
  author = {Lindsay, D. Stephen and Simons, Daniel J. and Lilienfeld, Scott O.},
  year = {2016},
  month = nov,
  volume = {29},
  abstract = {Psychological Science Editor in Chief D. Stephen Lindsay, Clinical Psychological Science Editor Scott O. Lilienfeld, and APS Fellow Daniel J. Simons explain the rationale for and benefits of preregistration, for researchers and for the field of psychological science at large.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\BVSBXU6L\\research-preregistration-101.html},
  journal = {APS Observer},
  language = {en-US},
  number = {10}
}

@article{martinArePsychologyJournals2017,
  title = {Are {{Psychology Journals Anti}}-Replication? {{A Snapshot}} of {{Editorial Practices}}},
  shorttitle = {Are {{Psychology Journals Anti}}-Replication?},
  author = {Martin, G. N. and Clarke, Richard M.},
  year = {2017},
  volume = {8},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.00523},
  abstract = {Recent research in psychology has highlighted a number of replication problems in the discipline, with publication bias \textendash{} the preference for publishing original and positive results, and a resistance to publishing negative results and replications- identified as one reason for replication failure. However, little empirical research exists to demonstrate that journals explicitly refuse to publish replications. We reviewed the instructions to authors and the published aims of 1151 psychology journals and examined whether they indicated that replications were permitted and accepted. We also examined whether journal practices differed across branches of the discipline, and whether editorial practices differed between low and high impact journals. Thirty three journals (3\%) stated in their aims or instructions to authors that they accepted replications. There was no difference between high and low impact journals. The implications of these findings for psychology are discussed.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\IB4FH5YA\\Martin and Clarke - 2017 - Are Psychology Journals Anti-replication A Snapsh.pdf},
  journal = {Frontiers in Psychology},
  keywords = {JOURNAL EDITORIAL PRACTICES,open developmental science,p-hacking,Psychology,Publication Bias,Replication},
  language = {English}
}

@article{mclean2019empirical,
  title = {The Empirical Structure of Narrative Identity: {{The}} Initial {{Big Three}}.},
  author = {McLean, Kate C and Syed, Moin and Pasupathi, Monisha and Adler, Jonathan M and Dunlop, William L and Drustrup, David and Fivush, Robyn and Graci, Matthew E and Lilgendahl, Jennifer P and {Lodi-Smith}, Jennifer and others},
  year = {2019},
  publisher = {{American Psychological Association}},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\XABPPFSJ\\McLean,etal,NarrativeStructure-inpressJPSP.pdf},
  journal = {Journal of personality and social psychology}
}

@article{moreauMetaanalysisTemplatesMaterials2019,
  title = {Meta-Analysis Templates and Materials},
  author = {Moreau, David and Gamble, Beau},
  year = {2019},
  month = nov,
  doi = {None},
  abstract = {Hosted on the Open Science Framework},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\ZI3WLI64\\q8stz.html},
  language = {en}
}

@misc{muenchenPopularityDataScience2012,
  title = {The {{Popularity}} of {{Data Science Software}}},
  author = {Muenchen, Robert A.},
  year = {2012},
  month = apr,
  abstract = {by Abstract This article, formerly known as The Popularity of Data Analysis Software, presents various ways of measuring the popularity or market share of software for advanced a\ldots},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\WEGIDWCV\\popularity.html},
  howpublished = {http://r4stats.com/articles/popularity/},
  journal = {r4stats.com},
  language = {en-US}
}

@book{national2009being,
  title = {On Being a Scientist: A Guide to Responsible Conduct in Research},
  author = {{National Academy of Sciences}},
  year = {2009},
  edition = {Third},
  publisher = {{National Academies Press (US)}},
  address = {{Washington, DC, US}}
}

@article{nosekPromotingOpenResearch2015a,
  title = {Promoting an Open Research Culture},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  year = {2015},
  month = jun,
  volume = {348},
  pages = {1422--1425},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility
Author guidelines for journals could help to promote transparency, openness, and reproducibility},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\7JEB688L\\Nosek et al_2015_Promoting an open research culture.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\ZKLD8GV8\\1422.html},
  journal = {Science},
  language = {en},
  number = {6242},
  pmid = {26113702}
}

@article{nosekScientificUtopiaOpening2012,
  title = {Scientific {{Utopia}}: {{I}}. {{Opening Scientific Communication}}},
  shorttitle = {Scientific {{Utopia}}},
  author = {Nosek, Brian A. and {Bar-Anan}, Yoav},
  year = {2012},
  month = jul,
  volume = {23},
  pages = {217--243},
  issn = {1047-840X},
  doi = {10.1080/1047840X.2012.692215},
  abstract = {Existing norms for scientific communication are rooted in anachronistic practices of bygone eras making them needlessly inefficient. We outline a path that moves away from the existing model of scientific communication to improve the efficiency in meeting the purpose of public science\textemdash knowledge accumulation. We call for six changes: (a) full embrace of digital communication; (b) open access to all published research; (c) disentangling publication from evaluation; (d) breaking the ``one article, one journal'' model with a grading system for evaluation and diversified dissemination outlets; (e) publishing peer review; and (f) allowing open, continuous peer review. We address conceptual and practical barriers to change and provide examples showing how the suggested practices are being used already. The critical barriers to change are not technical or financial; they are social. Although scientists guard the status quo, they also have the power to change it.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\PK3KU64G\\Nosek_Bar-Anan_2012_Scientific Utopia.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\JTSJWCMS\\1047840X.2012.html},
  journal = {Psychological Inquiry},
  number = {3}
}

@article{nowokSynthpopBespokeCreation2016,
  title = {Synthpop: {{Bespoke Creation}} of {{Synthetic Data}} in {{R}}},
  shorttitle = {Synthpop},
  author = {Nowok, Beata and Raab, Gillian M. and Dibben, Chris},
  year = {2016},
  month = oct,
  volume = {74},
  pages = {1--26},
  issn = {1548-7660},
  doi = {10.18637/jss.v074.i11},
  copyright = {Copyright (c) 2016 Beata Nowok, Gillian M. Raab, Chris Dibben},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\GVM22GB5\\Nowok et al_2016_synthpop.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\S9GYFN2L\\v074i11.html},
  journal = {Journal of Statistical Software},
  keywords = {CART,disclosure control,R,synthetic data,UK longitudinal studies},
  language = {en},
  number = {1}
}

@article{peikertReproducibleDataAnalysis2019,
  title = {A {{Reproducible Data Analysis Workflow}} with {{R Markdown}}, {{Git}}, {{Make}}, and {{Docker}}},
  author = {Peikert, Aaron and Brandmaier, Andreas Markus},
  year = {2019},
  month = nov,
  doi = {10.31234/osf.io/8xzqy},
  abstract = {In this tutorial, we describe a workflow to ensure long-term reproducibility of R-based data analyses. The workflow leverages established tools and practices from software engineering. It combines the benefits of various open-source software tools including R Markdown, Git, Make, and Docker, whose interplay ensures seamless integration of version management, dynamic report generation conforming to various journal styles and full cross-platform and long-term computational reproducibility. The workflow ensures meeting the primary goals that 1) the reporting of statistical results is consistent with the actual statistical results (dynamic report generation), the analysis exactly reproduces at a later time even if the computing platform or software is changed (computational reproducibility), and 3) changes at any time (during development and post-publication) are tracked, tagged, and documented while earlier versions of both data and code remain accessible.  While the research community increasingly recognizes dynamic document generation and version management as tools to ensure reproducibility, we demonstrate with practical examples that these alone are not sufficient to ensure long-term computational reproducibility. Leveraging containerization, dependence management, version management, and literate programming, the workflow increases scientific productivity by facilitating later reproducibility and reuse of code and data.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\AS9JL74K\\Peikert_Brandmaier_2019_A Reproducible Data Analysis Workflow with R Markdown, Git, Make, and Docker.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\2NT8MMMG\\8xzqy.html}
}

@misc{pengSimpleExplanationReplication2016,
  title = {A {{Simple Explanation}} for the {{Replication Crisis}} in {{Science}} {$\cdot$} {{Simply Statistics}}},
  author = {Peng, Roger},
  year = {2016},
  month = aug,
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\CMKAL9TN\\replication-crisis.html},
  journal = {Simplystats}
}

@article{plesserReproducibilityVsReplicability2018,
  title = {Reproducibility vs. {{Replicability}}: {{A Brief History}} of a {{Confused Terminology}}},
  shorttitle = {Reproducibility vs. {{Replicability}}},
  author = {Plesser, Hans E.},
  year = {2018},
  volume = {11},
  issn = {1662-5196},
  doi = {10.3389/fninf.2017.00076},
  abstract = {Reproducibility vs. Replicability: A Brief History of a Confused Terminology},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\VSY3D595\\Plesser_2018_Reproducibility vs.pdf},
  journal = {Frontiers in Neuroinformatics},
  keywords = {artifacts,computational science,repeatability,replicability,reproducibility},
  language = {English}
}

@article{ramGitCanFacilitate2013,
  title = {Git Can Facilitate Greater Reproducibility and Increased Transparency in Science},
  author = {Ram, Karthik},
  year = {2013},
  month = feb,
  volume = {8},
  pages = {7},
  issn = {1751-0473},
  doi = {10.1186/1751-0473-8-7},
  abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\RZB63RDE\\Ram_2013_Git can facilitate greater reproducibility and increased transparency in science.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\H4UH8INM\\1751-0473-8-7.html},
  journal = {Source Code for Biology and Medicine},
  number = {1}
}

@misc{rcoreteamLanguageEnvironmentStatistical2020,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {{R Core Team}},
  year = {2020},
  address = {{Vienna, Austria}},
  howpublished = {R Foundation for Statistical Computing}
}

@book{repro,
  title = {Repro: {{Automated}} Setup of Reproducible Workflows and Their Dependencies},
  author = {Peikert, Aaron and Brandmaier, Andreas Markus and Van Lissa, Caspar J.},
  year = {2020}
}

@misc{ReproducibleWorkflowVersion,
  title = {Reproducible Workflow and Version Control with {{Git}} and {{Github}}},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\7J672FN6\\git.html},
  howpublished = {https://jules32.github.io/2016-07-12-Oxford/git/}
}

@article{rooyenEffectPeerReview2010,
  title = {Effect on Peer Review of Telling Reviewers That Their Signed Reviews Might Be Posted on the Web: Randomised Controlled Trial},
  shorttitle = {Effect on Peer Review of Telling Reviewers That Their Signed Reviews Might Be Posted on the Web},
  author = {van Rooyen, Susan and Delamothe, Tony and Evans, Stephen J. W.},
  year = {2010},
  month = nov,
  volume = {341},
  pages = {c5729},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.c5729},
  abstract = {Objectives To see whether telling peer reviewers that their signed reviews of original research papers might be posted on the BMJ's website would affect the quality of their reviews.
Design Randomised controlled trial.
Setting A large international general medical journal based in the United Kingdom.
Participants 541 authors, 471 peer reviewers, and 12 editors.
Intervention Consecutive eligible papers were randomised either to have the reviewer's signed report made available on the BMJ's website alongside the published paper (intervention group) or to have the report made available only to the author\textemdash the BMJ's normal procedure (control group). The intervention was the act of revealing to reviewers\textemdash after they had agreed to review but before they undertook their review\textemdash that their signed report might appear on the website.
Main outcome measures The main outcome measure was the quality of the reviews, as independently rated on a scale of 1 to 5 using a validated instrument by two editors and the corresponding author. Authors and editors were blind to the intervention group. Authors rated review quality before the fate of their paper had been decided. Additional outcomes were the time taken to complete the review and the reviewer's recommendation regarding publication.
Results 558 manuscripts were randomised, and 471 manuscripts remained after exclusions. Of the 1039 reviewers approached to take part in the study, 568 (55\%) declined. Two editors' evaluations of the quality of the peer review were obtained for all 471 manuscripts, with the corresponding author's evaluation obtained for 453. There was no significant difference in review quality between the intervention and control groups (mean difference for editors 0.04, 95\% CI -0.09 to 0.17; for authors 0.06, 95\% CI -0.09 to 0.20). Any possible difference in favour of the control group was well below the level regarded as editorially significant. Reviewers in the intervention group took significantly longer to review (mean difference 25 minutes, 95\% CI 3.0 to 47.0 minutes).
Conclusion Telling peer reviewers that their signed reviews might be available in the public domain on the BMJ's website had no important effect on review quality. Although the possibility of posting reviews online was associated with a high refusal rate among potential peer reviewers and an increase in the amount of time taken to write a review, we believe that the ethical arguments in favour of open peer review more than outweigh these disadvantages.},
  copyright = {\textcopyright{} van Rooyen et al 2010. This is an open-access article distributed under the terms of the Creative Commons Attribution Non-commercial License, which permits use, distribution, and reproduction in any medium, provided the original work is properly cited, the use is non commercial and is otherwise in compliance with the license. See: http://creativecommons.org/licenses/by-nc/2.0/  and  http://creativecommons.org/licenses/by-nc/2.0/legalcode.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\4LP5VZPE\\Rooyen et al_2010_Effect on peer review of telling reviewers that their signed reviews might be.pdf},
  journal = {BMJ},
  language = {en},
  pmid = {21081600}
}

@article{rosenbergTidyLPAPackageEasily2018,
  title = {{{tidyLPA}}: {{An R Package}} to {{Easily Carry Out Latent Profile Analysis}} ({{LPA}}) {{Using Open}}-{{Source}} or {{Commercial Software}}},
  shorttitle = {{{tidyLPA}}},
  author = {Rosenberg, Joshua and Beymer, Patrick and Anderson, Daniel and Van Lissa, Caspar J. and Schmidt, Jennifer},
  year = {2018},
  month = oct,
  volume = {3},
  pages = {978},
  issn = {2475-9066},
  doi = {10.21105/joss.00978},
  abstract = {Researchers are often interested in identifying homogeneous subgroups within heterogeneous samples on the basis of a set of measures, such as profiles of individuals' motivation (i.e., their values, competence beliefs, and achievement goals). Latent Profile Analysis (LPA) is a statistical method for identifying such groups, or latent profiles, and is a special case of the general mixture model where all measured variables are continuous (Harring \& Hodis, 2016; Pastor, Barron, Miller, \& Davis, 2007). The tidyLPA package allows users to specify different models that determine whether and how different parameters (i.e., means, variances, and covariances) are estimated, and to specify and compare different solutions based on the number of profiles extracted.},
  copyright = {All rights reserved},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\JVGDVPE9\\Rosenberg et al. - 2018 - tidyLPA An R Package to Easily Carry Out Latent P.pdf},
  journal = {Journal of Open Source Software},
  language = {en},
  number = {30}
}

@article{ross-hellauerWhatOpenPeer2017,
  title = {What Is Open Peer Review? {{A}} Systematic Review},
  shorttitle = {What Is Open Peer Review?},
  author = {{Ross-Hellauer}, Tony},
  year = {2017},
  month = aug,
  volume = {6},
  issn = {2046-1402},
  doi = {10.12688/f1000research.11369.2},
  abstract = {Background: ``Open peer review'' (OPR), despite being a major pillar of Open Science, has neither a standardized definition nor an agreed schema of its features and implementations. The literature reflects this, with numerous overlapping and contradictory definitions. While for some the term refers to peer review where the identities of both author and reviewer are disclosed to each other, for others it signifies systems where reviewer reports are published alongside articles. For others it signifies both of these conditions, and for yet others it describes systems where not only ``invited experts'' are able to comment. For still others, it includes a variety of combinations of these and other novel methods., 
Methods: Recognising the absence of a consensus view on what open peer review is, this article undertakes a systematic review of definitions of ``open peer review'' or ``open review'', to create a corpus of 122 definitions. These definitions are systematically analysed to build a coherent typology of the various innovations in peer review signified by the term, and hence provide the precise technical definition currently lacking., 
Results: This quantifiable data yields rich information on the range and extent of differing definitions over time and by broad subject area. Quantifying definitions in this way allows us to accurately portray exactly how ambiguously the phrase ``open peer review'' has been used thus far, for the literature offers 22 distinct configurations of seven traits, effectively meaning that there are 22 different definitions of OPR in the literature reviewed., 
Conclusions: I propose a pragmatic definition of open peer review as an umbrella term for a number of overlapping ways that peer review models can be adapted in line with the aims of Open Science, including making reviewer and author identities open, publishing review reports and enabling greater participation in the peer review process.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\IRD7VFHQ\\Ross-Hellauer_2017_What is open peer review.pdf},
  journal = {F1000Research},
  pmcid = {PMC5437951},
  pmid = {28580134}
}

@misc{rossRedocReversibleReproducible2020,
  title = {Redoc - {{Reversible Reproducible Documents}}},
  author = {Ross, Noam},
  year = {2020},
  month = jan,
  abstract = {Reversible Reproducible Documents. Contribute to noamross/redoc development by creating an account on GitHub.}
}

@book{rstudioteamRStudioIntegratedDevelopment2015,
  title = {{{RStudio}}: {{Integrated}} Development Environment for {{R}}},
  author = {{RStudio Team}},
  year = {2015},
  address = {{Boston, MA}},
  organization = {{RStudio, Inc.}}
}

@article{shroutPsychologyScienceKnowledge2018,
  title = {Psychology, {{Science}}, and {{Knowledge Construction}}: {{Broadening Perspectives}} from the {{Replication Crisis}}},
  shorttitle = {Psychology, {{Science}}, and {{Knowledge Construction}}},
  author = {Shrout, Patrick E. and Rodgers, Joseph L.},
  year = {2018},
  volume = {69},
  pages = {487--510},
  doi = {10.1146/annurev-psych-122216-011845},
  abstract = {Psychology advances knowledge by testing statistical hypotheses using empirical observations and data. The expectation is that most statistically significant findings can be replicated in new data and in new laboratories, but in practice many findings have replicated less often than expected, leading to claims of a replication crisis. We review recent methodological literature on questionable research practices, meta-analysis, and power analysis to explain the apparently high rates of failure to replicate. Psychologists can improve research practices to advance knowledge in ways that improve replicability. We recommend that researchers adopt open science conventions of preregi-stration and full disclosure and that replication efforts be based on multiple studies rather than on a single replication attempt. We call for more sophisticated power analyses, careful consideration of the various influences on effect sizes, and more complete disclosure of nonsignificant as well as statistically significant findings.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\VSW7FA4W\\Shrout and Rodgers - 2018 - Psychology, Science, and Knowledge Construction B.pdf},
  journal = {Annual Review of Psychology},
  number = {1},
  pmid = {29300688}
}

@article{stanleyReproducibleTablesPsychology2018,
  title = {Reproducible {{Tables}} in {{Psychology Using}} the {{apaTables Package}}},
  author = {Stanley, David J. and Spence, Jeffrey R.},
  year = {2018},
  month = sep,
  volume = {1},
  pages = {415--431},
  issn = {2515-2459},
  doi = {10.1177/2515245918773743},
  abstract = {Growing awareness of how susceptible research is to errors, coupled with well-documented replication failures, has caused psychological researchers to move toward open science and reproducible research. In this Tutorial, to facilitate reproducible psychological research, we present a tool that creates reproducible tables that follow the American Psychological Association's (APA's) style. Our tool, apaTables, automates the creation of APA-style tables for commonly used statistics and analyses in psychological research: correlations, multiple regressions (with and without blocks), standardized mean differences, N-way independent-groups analyses of variance (ANOVAs), within-subjects ANOVAs, and mixed-design ANOVAs. All tables are saved as Microsoft Word documents, so they can be readily incorporated into manuscripts without manual formatting or transcription of values.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\SYWRI2PD\\Stanley_Spence_2018_Reproducible Tables in Psychology Using the apaTables Package.pdf},
  journal = {Advances in Methods and Practices in Psychological Science},
  keywords = {data sharing,open data,open materials,open science,R,replication,reproducibility,reproducible analyses,reproducible research,reproducible tables,statistical tools,transparency in research},
  language = {en},
  number = {3}
}

@misc{SupplementalUsingGit,
  title = {Supplemental: {{Using Git}} from {{RStudio}} \textendash{} {{Version Control}} with {{Git}}},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\7XNF7PKP\\index.html},
  howpublished = {https://swcarpentry.github.io/git-novice/14-supplemental-rstudio/index.html}
}

@misc{syedPromiseOpenScience2019,
  title = {The {{Promise}} of the {{Open Science Movement}} for {{Research}} on {{Identity}}},
  author = {Syed, Moin},
  year = {2019},
  month = may,
  address = {{Naples, Italy}},
  abstract = {The open science movement has been gaining steam in numerous scientific disciplines (e.g., ecology, cancer biology, economics) as well as sub-disciplines of psychology (e.g., social, personality). These issues, however, have been scantly discussed in the context of identity research. This presentation will include an overview of core issues in the open science movement and how they apply to research on identity. Emphasis will be placed on how incorporating open science principles can improve both theoretical and empirical work on identity.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\GTZZZWLT\\Syed - The Promise of the Open Science Movement for Resea.pdf},
  language = {en},
  type = {Presidential Address}
}

@misc{TechBlogGitReproducibility,
  title = {{{TechBlog}}: {{Git}}: {{The}} Reproducibility Tool Scientists Love to Hate : {{Naturejobs Blog}}},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\UA5Y7BXZ\\git-the-reproducibility-tool-scientists-love-to-hate.html},
  howpublished = {http://blogs.nature.com/naturejobs/2018/06/11/git-the-reproducibility-tool-scientists-love-to-hate/}
}

@misc{tennantOpenScienceJust2018,
  title = {Open Science Is Just Good Science},
  author = {Tennant, Jonathan},
  year = {2018},
  address = {{TU Delft}}
}

@techreport{tennantValuePropositionOpen2020,
  title = {A Value Proposition for {{Open Science}}},
  author = {Tennant, Jonathan},
  year = {2020},
  month = mar,
  institution = {{SocArXiv}},
  doi = {10.31235/osf.io/k9qhv},
  abstract = {Open Science has become commonly understood in terms of its practices. Open Access, Open Data, and Open Source software are all becoming commonplace in academia. However, unlike the Free and Open Source Software movement, Open Science seems to have become largely divorced from its pluralistic philosophical and ethical foundations, which seem to have reignited from the humanities at the turn of the Millennium. To close this gap, I propose a new value-based proposition for Open Science, that is akin to the ``four fundamental freedoms'' of Richard Stallman that catalysed the Free Software movement. In doing so, I hope to provide a more common, unified, and human understanding to notions of openness in science.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\EB4V8JX6\\Tennant - 2020 - A value proposition for Open Science.pdf},
  language = {en},
  type = {Preprint}
}

@article{vantveerPreregistrationSocialPsychology2016,
  title = {Pre-Registration in Social Psychology\textemdash{{A}} Discussion and Suggested Template},
  author = {{van 't Veer}, Anna Elisabeth and {Giner-Sorolla}, Roger},
  year = {2016},
  month = nov,
  volume = {67},
  pages = {2--12},
  issn = {00221031},
  doi = {10.1016/j.jesp.2016.03.004},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\4CHWSIJB\\van 't Veer_Giner-Sorolla_2016_Pre-registration in social psychologyA discussion and suggested template.pdf},
  journal = {Journal of Experimental Social Psychology},
  language = {en}
}

@article{walshOpenPeerReview2000,
  title = {Open Peer Review: {{A}} Randomised Controlled Trial},
  shorttitle = {Open Peer Review},
  author = {Walsh, Elizabeth and Rooney, Maeve and Appleby, Louis and Wilkinson, Greg},
  year = {2000},
  month = jan,
  volume = {176},
  pages = {47--51},
  issn = {0007-1250, 1472-1465},
  doi = {10.1192/bjp.176.1.47},
  abstract = {Background
Most scientific journals practise anonymous peer review. There is no evidence, however, that this is any better than an open system.

Aims
To evaluate the feasibility of an open peer review system.

Method
Reviewers for the British Journal of Psychiatry were asked whether they would agree to have their name revealed to the authors whose papers they review; 408 manuscripts assigned to reviewers who agreed were randomised to signed or unsigned groups. We measured review quality, tone, recommendation for publication and time taken to complete each review.

Results
A total of 245 reviewers (76\%) agreed to sign. Signed reviews were of higher quality, were more courteous and took longer to complete than unsigned reviews. Reviewers who signed were more likely to recommend publication.

Conclusions
This study supports the feasibility of an open peer review system and identifies such a system's potential drawbacks.},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\LT9ETGKG\\Walsh et al_2000_Open peer review.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\YZMR76SL\\1F81447FC67B3BAFDCCCCE82B6C7A187.html},
  journal = {The British Journal of Psychiatry},
  language = {en},
  number = {1}
}

@article{westonRecommendationsIncreasingTransparency2019,
  title = {Recommendations for {{Increasing}} the {{Transparency}} of {{Analysis}} of {{Preexisting Data Sets}}},
  shorttitle = {Recommendations for {{Increasing}} the {{Transparency}} of {{Analysis}} of {{Preexisting Data Sets}}},
  author = {Weston, Sara J. and Ritchie, Stuart J. and Rohrer, Julia M. and Przybylski, Andrew K.},
  year = {2019},
  month = jun,
  doi = {10.1177/2515245919848684},
  abstract = {Secondary data analysis, or the analysis of preexisting data, provides a powerful tool for the resourceful psychological scientist. Never has this been more tru...},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\DE7WSWWM\\Weston et al_2019_Recommendations for Increasing the Transparency of Analysis of Preexisting Data.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\IX2RRN5Z\\2515245919848684.html},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en}
}

@article{wickhamTidyData2014,
  title = {Tidy {{Data}}},
  author = {Wickham, Hadley},
  year = {2014},
  month = sep,
  volume = {59},
  pages = {1--23},
  issn = {1548-7660},
  doi = {10.18637/jss.v059.i10},
  copyright = {Copyright (c) 2013 Hadley  Wickham},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\XJ5UTIN7\\Wickham_2014_Tidy Data.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\VR53BUBH\\v059i10.html},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {1}
}

@article{wilkinsonFAIRGuidingPrinciples2016a,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and {da Silva Santos}, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and {Gonzalez-Beltran}, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and {'t Hoen}, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and {Rocca-Serra}, Philippe and Roos, Marco and {van Schaik}, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and {van der Lei}, Johan and {van Mulligen}, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  year = {2016},
  month = dec,
  volume = {3},
  pages = {160018},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\EYCPVCGR\\Wilkinson et al_2016_The FAIR Guiding Principles for scientific data management and stewardship.pdf},
  journal = {Scientific Data},
  language = {en},
  number = {1}
}

@article{wrightRangerFastImplementation2015,
  title = {Ranger: {{A Fast Implementation}} of {{Random Forests}} for {{High Dimensional Data}} in {{C}}++ and {{R}}},
  shorttitle = {Ranger},
  author = {Wright, Marvin N. and Ziegler, Andreas},
  year = {2015},
  month = aug,
  abstract = {We introduce the C++ application and R package ranger. The software is a fast implementation of random forests for high dimensional data. Ensembles of classification, regression and survival trees are supported. We describe the implementation, provide examples, validate the package with a reference implementation, and compare runtime and memory usage with other implementations. The new software proves to scale best with the number of features, samples, trees, and features tried for splitting. Finally, we show that ranger is the fastest and most memory efficient implementation of random forests to analyze data on the scale of a genome-wide association study.},
  archivePrefix = {arXiv},
  eprint = {1508.04409},
  eprinttype = {arxiv},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\2QCM8EU7\\Wright_Ziegler_2015_ranger.pdf;C\:\\Users\\lissa102\\Zotero\\storage\\W7MZ7V4J\\1508.html},
  journal = {arXiv:1508.04409 [stat]},
  keywords = {Statistics - Computation,Statistics - Machine Learning},
  primaryClass = {stat}
}

@article{yamadaHowCrackPreregistration2018,
  title = {How to {{Crack Pre}}-Registration: {{Toward Transparent}} and {{Open Science}}},
  shorttitle = {How to {{Crack Pre}}-Registration},
  author = {Yamada, Yuki},
  year = {2018},
  month = sep,
  volume = {9},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2018.01831},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\TBFHD98N\\Yamada - 2018 - How to Crack Pre-registration Toward Transparent .pdf},
  journal = {Frontiers in Psychology},
  language = {en}
}

@misc{ZombieLiterature,
  title = {The {{Zombie Literature}}},
  abstract = {Retractions are on the rise. But reams of flawed research papers persist in the scientific literature. Is it time to change the way papers are published?},
  file = {C\:\\Users\\lissa102\\Zotero\\storage\\HQTCXNXY\\the-zombie-literature-33627.html},
  howpublished = {https://www.the-scientist.com/features/the-zombie-literature-33627},
  journal = {The Scientist Magazine\textregistered},
  language = {en}
}


